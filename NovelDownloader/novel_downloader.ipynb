{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e45e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import types\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAX_THREADS = 30\n",
    "semaphore = threading.Semaphore(MAX_THREADS)\n",
    "event = threading.Event()\n",
    "\n",
    "\n",
    "class download(object):\n",
    "    def __init__(self, target):\n",
    "        self.__target_url = target\n",
    "        self.__head = {\n",
    "            'User-Agent':\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36 Edg/108.0.1462.76',\n",
    "        }\n",
    "        self.__proxy = {\n",
    "            'http': 'http://{}'.format('127.0.0.1:7892'),\n",
    "            'https': 'https://{}'.format('127.0.0.1:7893'),\n",
    "        }\n",
    "        self.url = \"https://www.soquwu.com\"\n",
    "        self.final_text = {}\n",
    "        self.finished_num = 0\n",
    "        self.total_num = 0\n",
    "\n",
    "    def get_download_url(self, url):\n",
    "        charter = re.compile(u'[第弟](.+)章', re.IGNORECASE)\n",
    "        target_req = request.Request(url=url, headers=self.__head)\n",
    "        target_response = request.urlopen(target_req)\n",
    "        target_html = target_response.read().decode('gbk', 'ignore')\n",
    "        listmain_soup = BeautifulSoup(target_html, 'lxml')\n",
    "        title = listmain_soup.find_all('div', id='info')\n",
    "        if len(title) == 0:\n",
    "            self.GetRealUrl(-1, listmain_soup)\n",
    "        else:\n",
    "            title_soup = BeautifulSoup(str(title), 'lxml')\n",
    "            novel_name = str(title_soup.h1).split(\">\")[1][:-4]\n",
    "            flag_name = \"《\" + novel_name + \"》\" + \"正文\"\n",
    "\n",
    "            chapters = listmain_soup.find_all('div', id='list')\n",
    "            download_soup = BeautifulSoup(str(chapters), 'lxml')\n",
    "            numbers = (len(download_soup.dl.contents) - 1) / 2 - 8\n",
    "            download_dict = collections.OrderedDict()\n",
    "            begin_flag = False\n",
    "            numbers = 1\n",
    "            for child in download_soup.dl.children:\n",
    "                if child != '\\n':\n",
    "                    if child.string == u\"%s\" % flag_name:\n",
    "                        begin_flag = True\n",
    "                    if begin_flag == True and child.a != None:\n",
    "                        download_url = self.url + child.a.get('href')\n",
    "                        download_name = child.string\n",
    "                        name = str(download_name)\n",
    "                        #print(numbers)\n",
    "                        #import pdb;pdb.set_trace()\n",
    "                        download_dict[name] = download_url\n",
    "                        numbers += 1\n",
    "            self.total_num = numbers\n",
    "            return novel_name + '.txt', download_dict\n",
    "\n",
    "    def Downloader(self, charpter_num, chapter_name, url):\n",
    "        with semaphore:\n",
    "            text = self.get_text(charpter_num, url)\n",
    "            self.final_text[chapter_name] = {\n",
    "                \"index\": charpter_num,\n",
    "                \"content\": text\n",
    "            }\n",
    "        event.set()\n",
    "\n",
    "    def get_text(self, charter_num, url, index=0):\n",
    "        download_req = request.Request(url=url, headers=self.__head)\n",
    "        download_response = request.urlopen(download_req)\n",
    "        download_html = download_response.read().decode('gbk', 'ignore')\n",
    "        soup_texts = BeautifulSoup(download_html, 'lxml')\n",
    "        texts = soup_texts.find_all(id='content')\n",
    "\n",
    "        try:\n",
    "            soup_text = BeautifulSoup(str(texts),\n",
    "                                      'lxml').div.text.replace('app2();', '')\n",
    "        except:\n",
    "            if (index == 0):\n",
    "                print(f\"章节{charter_num}错误 重试第\" + str(index + 1) + \"次\")\n",
    "                return self.GetRealUrl(charter_num, str(soup_texts), index)\n",
    "            else:\n",
    "                print(\"章节再次错误\")\n",
    "                soup_text = \"\"\n",
    "        soup_text = re.sub(u'[\\'(https].*[.com]', '', soup_text)\n",
    "        soup_text = re.sub(u'([\\s][\\s])', '\\n\\n', soup_text)\n",
    "        self.finished_num += 1\n",
    "        sys.stdout.write(\"已下载:%.3f%%\" %\n",
    "                         float(self.finished_num * 100 / self.total_num) +\n",
    "                         '\\r')\n",
    "        sys.stdout.flush()\n",
    "        return soup_text\n",
    "\n",
    "    def Writer(self, name):\n",
    "        path = f\"./novels/{name}\"\n",
    "        if name in os.listdir(\"./novels/\"):\n",
    "            os.remove(path)\n",
    "        content_dic = sorted(self.final_text.items(),\n",
    "                             key=lambda k: (k[1][\"index\"]))\n",
    "        with open(path, 'a', encoding='utf-8') as f:\n",
    "            for chapter_name, text in content_dic:\n",
    "                write_flag = True\n",
    "                f.write(chapter_name + '\\n\\n')\n",
    "                for each in text[\"content\"]:\n",
    "                    if each == 'h':\n",
    "                        write_flag = False\n",
    "                    if write_flag == True and each != ' ':\n",
    "                        f.write(each)\n",
    "                    if write_flag == True and each == '\\r':\n",
    "                        f.write('\\n')\n",
    "                f.write('\\n\\n')\n",
    "\n",
    "    def GetRealUrl(self, charter_num, soup_texts, index=0):\n",
    "        p = re.compile(u'\\'[\\w!@#$%^&/.=?]*\\'\"')\n",
    "        p2 = re.compile(u'\\'[0-9\\w!@#$%&/.=?]+\\'')\n",
    "        s = str(soup_texts)\n",
    "        strings = re.findall(p2, s)\n",
    "        url = \"\"\n",
    "        for ss in strings:\n",
    "            url = ss.strip('\\'') + url\n",
    "        url = self.url + url\n",
    "        print(url)\n",
    "        if charter_num != -1:\n",
    "            return self.get_text(charter_num, url, index + 1)\n",
    "        else:\n",
    "            return self.get_download_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56af9b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t欢迎使用《笔趣看》小说下载小工具\n",
      "\n",
      "\t\t作者:potato\t时间:2020-2-13\n",
      "\n",
      "*************************************************************************\n",
      "《深海余烬》下载中:\n",
      "《深海余烬》下载完成！\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\n",
    "        \"\\n\\t\\t欢迎使用《搜趣屋》小说下载小工具\\n\\n\\t\\t搜趣屋网址: https://www.soquwu.com\\n\\n\\t\\t作者:potato\\t时间:2020-2-24\\n\"\n",
    "    )\n",
    "    print(\n",
    "        \"*************************************************************************\"\n",
    "    )\n",
    "    target_url = str(input(\"请输入小说目录下载地址:\\n\"))\n",
    "    # target_url = \"https://www.soquwu.com/90_90897/\"\n",
    "    d = download(target=target_url)\n",
    "    name, url_dict = d.get_download_url(target_url)\n",
    "\n",
    "    index = 1\n",
    "\n",
    "    print(\"《%s》下载中:\" % name[:-4])\n",
    "    threads = []\n",
    "    index = 1\n",
    "    lock = threading.Lock()\n",
    "    for chapter_name, chapter_url in url_dict.items():\n",
    "        thread = threading.Thread(target=d.Downloader,\n",
    "                                  args=(index, chapter_name, chapter_url))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "        index += 1\n",
    "\n",
    "    # total_size = len(url_dict)\n",
    "    # with tqdm(total=total_size, unit=\"B\", unit_scale=True) as pbar:\n",
    "    #     progress = d.finished_index\n",
    "    #     while progress < total_size:\n",
    "    #         progress = d.finished_index\n",
    "    #         pbar.update(progress - pbar.n)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    d.Writer(name)\n",
    "    print(\"《%s》下载完成！\" % name[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae5067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5e185f390bca09bdd7db73b6a71f74f807c94716acb67dd2a016aa3cb5c8edb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
